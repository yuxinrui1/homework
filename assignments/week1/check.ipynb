{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db436060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    A linear regression model that closed form to fit the model.\n",
    "    \"\"\"\n",
    "\n",
    "    w: np.ndarray\n",
    "    b: float\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = np.ndarray\n",
    "        self.b = float\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> (None):\n",
    "        \"\"\"\n",
    "        fit the function by closed form\n",
    "\n",
    "         Arguments:\n",
    "             X (np.ndarray): The input data.\n",
    "             y (np.ndarray): The input data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "         Returns:\n",
    "             None\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n = y.shape[0]\n",
    "        p = X.shape[1]\n",
    "        self.index = np.ones((n, 1), dtype=int)\n",
    "        X = np.column_stack((X, self.index))\n",
    "        self.beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        self.b = self.beta_hat[p]\n",
    "        self.w = self.beta_hat[0:p]\n",
    "        print(self.w)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> (np.ndarray):\n",
    "        \"\"\"\n",
    "        Predict the output for the given input.\n",
    "\n",
    "        Arguments:\n",
    "            X (np.ndarray): The input data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The predicted output.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y = np.matmul(X, self.w) + self.b\n",
    "        return y\n",
    "\n",
    "\n",
    "class GradientDescentLinearRegression(LinearRegression):\n",
    "    \"\"\"\n",
    "    A linear regression model that uses gradient descent to fit the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(\n",
    "        self, X: np.ndarray, y: np.ndarray, lr: float = 0.01, epochs: int = 1000\n",
    "    ) -> (np.ndarray):\n",
    "        \"\"\"\n",
    "        fit the function by gradient descent\n",
    "\n",
    "         Arguments:\n",
    "             X (np.ndarray): The input data.\n",
    "             y (np.ndarray): The input data\n",
    "             lr (float): learning rate\n",
    "             epochs (int): number of epochs\n",
    "\n",
    "\n",
    "\n",
    "         Returns:\n",
    "             np.ndarray: The fitted value output.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros((n, 1))\n",
    "        self.bias = np.zeros((1))\n",
    "        y = y.reshape(m, 1)\n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "\n",
    "            y_hat = np.matmul(X, self.weights) + self.bias\n",
    "\n",
    "            # Calculting loss\n",
    "            loss = np.mean((y_hat - y) ** 2)\n",
    "\n",
    "            # Appending loss in list: losses\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Calculating derivatives of parameters(weights, and\n",
    "            # bias)\n",
    "            dw = (2 / m) * np.matmul(X.T, (y_hat - y))\n",
    "            db = (2 / m) * np.sum((y_hat - y))\n",
    "            # Updating the parameters: parameter := parameter - lr*derivative\n",
    "            # of loss/cost w.r.t parameter)\n",
    "            self.weights -= lr * dw\n",
    "            self.bias -= lr * db\n",
    "\n",
    "            # y = np.matmul(X, self.weights) + self.bias\n",
    "        y_fitted = np.matmul(X, self.weights) + self.bias\n",
    "        self.out = loss\n",
    "\n",
    "        # print(loss)\n",
    "        return y_fitted\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the output for the given input.\n",
    "\n",
    "        Arguments:\n",
    "            X (np.ndarray): The input data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The predicted output.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        product = np.matmul(X, self.weights) + self.bias\n",
    "        return product\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc909a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unittests for linear regression.\n",
    "\n",
    "Test that the linear regression from `model.py` works. Remember that this is\n",
    "only a subset of the unittests that will run on your code; the instructors have\n",
    "a holdout test suite that will be used to evaluate your code.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import inspect\n",
    "import numpy as np\n",
    "from model import LinearRegression, GradientDescentLinearRegression\n",
    "import pytest\n",
    "\n",
    "model_parametrize = pytest.mark.parametrize(\n",
    "    \"model\", [LinearRegression, GradientDescentLinearRegression]\n",
    ")\n",
    "\n",
    "\n",
    "@model_parametrize\n",
    "def test_has_correct_attributes(model):\n",
    "    \"\"\"\n",
    "    Test that the LinearRegression class has the correct attributes.\n",
    "    \"\"\"\n",
    "    lr = model()\n",
    "    assert hasattr(lr, \"w\"), f\"{str(model)} does not have attribute `w`.\"\n",
    "    assert hasattr(lr, \"b\"), f\"{str(model)} does not have attribute `b`.\"\n",
    "    assert hasattr(lr, \"fit\"), f\"{str(model)} does not have method `fit`.\"\n",
    "    assert hasattr(lr, \"predict\"), f\"{str(model)} does not have method `predict`.\"\n",
    "\n",
    "\n",
    "@model_parametrize\n",
    "def test_fn_signatures(model):\n",
    "    \"\"\"\n",
    "    Disallow untyped signatures.\n",
    "\n",
    "    \"\"\"\n",
    "    from inspect import signature\n",
    "\n",
    "    lr = model()\n",
    "    # all methods' arguments and returns must be typed.\n",
    "    methods = [\"fit\", \"predict\"]\n",
    "    for method in methods:\n",
    "        assert (\n",
    "            signature(getattr(lr, method)).return_annotation is not inspect._empty\n",
    "        ), f\"The return type of `{method}` is not annotated.\"\n",
    "\n",
    "        # Arguments must be typed.\n",
    "        for param in signature(getattr(lr, method)).parameters.values():\n",
    "            assert (\n",
    "                param.annotation is not inspect._empty\n",
    "            ), f\"The argument type of `{method}:{param.name}` is not annotated.\"\n",
    "\n",
    "\n",
    "@model_parametrize\n",
    "def test_docstrings(model):\n",
    "    \"\"\"\n",
    "    Disallow missing docstrings.\n",
    "\n",
    "    \"\"\"\n",
    "    lr = model()\n",
    "    # all methods must have a docstring.\n",
    "    methods = [\"fit\", \"predict\"]\n",
    "    for method in methods:\n",
    "        assert (\n",
    "            getattr(lr, method).__doc__ is not None\n",
    "        ), f\"The method `{method}` does not have a docstring.\"\n",
    "\n",
    "    # all classes must have a docstring.\n",
    "    classes = [model]\n",
    "    for class_ in classes:\n",
    "        assert (\n",
    "            class_.__doc__ is not None\n",
    "        ), f\"The class `{class_}` does not have a docstring.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_epochs_improve_fit():\n",
    "    \"\"\"\n",
    "    Test that GradientDescentLinearRegression improves with more epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def mse(y, y_hat):\n",
    "        return np.mean((y - y_hat) ** 2)\n",
    "\n",
    "    X = np.array([[1, 2], [3, 4]])\n",
    "    y = np.array([1, 2])\n",
    "\n",
    "    lr = GradientDescentLinearRegression()\n",
    "    lr.fit(X, y, epochs=10)\n",
    "    mse1 = mse(y, lr.predict(X))\n",
    "\n",
    "    lr = GradientDescentLinearRegression()\n",
    "    lr.fit(X, y, epochs=1000)\n",
    "    mse2 = mse(y, lr.predict(X))\n",
    "\n",
    "    assert mse1 > mse2, \"MSE should improve with more epochs.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7d09181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_closed_form_fit():\n",
    "    \"\"\"\n",
    "    Test that the LinearRegression class fits the model correctly.\n",
    "    \"\"\"\n",
    "    lr = LinearRegression()\n",
    "    X = np.array([[1, 2], [3, 4]])\n",
    "    y = np.array([1, 2])\n",
    "    lr.fit(X, y)\n",
    "    assert np.allclose(lr.w, np.array([0, 0.5])), \"Incorrect value for `w`.\"\n",
    "    assert np.allclose(lr.b, 0), \"Incorrect value for `b`.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0013779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.5]\n"
     ]
    }
   ],
   "source": [
    "test_closed_form_fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
